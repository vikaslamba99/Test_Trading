{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f06c29-9c45-4b5a-8dad-73687a9bceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.datasets import fetch_openml\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195160f5-eb68-479f-ad1d-13dd226412bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e48dd2-e54c-4d9e-a4f7-4bc619873766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/Volumes/My Book/Data_Analysis/Nasdaq') # set to e.g. external harddrive\n",
    "DATA_STORE = data_path/'assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98885c41-dc80-4634-8e10-693e8f2b2d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 15389314 entries, (Timestamp('1962-01-02 00:00:00'), 'ARNC') to (Timestamp('2018-03-27 00:00:00'), 'ZUMZ')\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count     Dtype  \n",
      "---  ------       --------------     -----  \n",
      " 0   open         15388776 non-null  float64\n",
      " 1   high         15389259 non-null  float64\n",
      " 2   low          15389259 non-null  float64\n",
      " 3   close        15389313 non-null  float64\n",
      " 4   volume       15389314 non-null  float64\n",
      " 5   ex-dividend  15389314 non-null  float64\n",
      " 6   split_ratio  15389313 non-null  float64\n",
      " 7   adj_open     15388776 non-null  float64\n",
      " 8   adj_high     15389259 non-null  float64\n",
      " 9   adj_low      15389259 non-null  float64\n",
      " 10  adj_close    15389313 non-null  float64\n",
      " 11  adj_volume   15389314 non-null  float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 1.4+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = (pd.read_csv(data_path/'wiki_prices.csv',\n",
    "                 parse_dates=['date'],\n",
    "                 index_col=['date', 'ticker'],\n",
    "                 infer_datetime_format=True)\n",
    "     .sort_index())\n",
    "\n",
    "print(df.info(show_counts=True))\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('quandl/wiki/prices', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2dcf46-5559-44ec-a947-bdc73652c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3199 entries, 0 to 3198\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   code    3199 non-null   object\n",
      " 1   name    3199 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 50.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path/'wiki_stocks.csv')\n",
    "# no longer needed\n",
    "# df = pd.concat([df.loc[:, 'code'].str.strip(),\n",
    "#                 df.loc[:, 'name'].str.split('(', expand=True)[0].str.strip().to_frame('name')], axis=1)\n",
    "\n",
    "print(df.info(show_counts=True))\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('quandl/wiki/stocks', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "692cea1d-c54f-498d-88c2-1e6ec531e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close      Volume\n",
      "Date                                                                              \n",
      "2000-01-03  792.830017  798.429993  778.200012  788.789978  788.789978   931800000\n",
      "2000-01-04  788.789978  788.789978  757.750000  759.010010  759.010010  1009000000\n",
      "2000-01-05  759.010010  768.229980  747.969971  761.520020  761.520020  1085500000\n",
      "2000-01-06  761.520020  767.659973  756.559998  762.640015  762.640015  1092300000\n",
      "2000-01-07  762.640015  783.510010  759.330017  783.489990  783.489990  1225200000                    Open         High          Low        Close    Adj Close      Volume\n",
      "Date                                                                                   \n",
      "2024-01-26  2313.659912  2324.020020  2311.610107  2316.320068  2316.320068  3353400000\n",
      "2024-01-29  2318.689941  2335.350098  2314.699951  2334.239990  2334.239990  3525160000\n",
      "2024-01-30  2335.310059  2336.050049  2326.370117  2330.679932  2330.679932  3836130000\n",
      "2024-01-31  2312.870117  2320.649902  2288.239990  2288.469971  2288.469971  4696120000\n",
      "2024-02-01  2297.360107  2317.340088  2296.159912  2316.800049  2316.800049  4386090000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = web.DataReader(name='SP500', data_source='fred', start=2009).squeeze().to_frame('close')\n",
    "print(df.info())\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sp500/fred', df)\n",
    "\n",
    "\n",
    "The below code needs to be taken to a separate notebook and used to source SP500 index data and save in store,\n",
    "because the yf_override() call disables the web.DataReader function to be used for other data sources.\n",
    "\n",
    "\"\"\"\n",
    "yf.pdr_override()\n",
    "df = pd.DataFrame(web.get_data_yahoo('^OEX', start='2000-01-01', end='2024-02-02'))\n",
    "print(df.head(), df.tail())\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sp500/fred', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdcdfe67-89f0-4eb7-a74b-7bb368a76d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 18734 entries, 1950-01-03 to 2024-02-09\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   open    18734 non-null  float64\n",
      " 1   high    18734 non-null  float64\n",
      " 2   low     18734 non-null  float64\n",
      " 3   close   18734 non-null  float64\n",
      " 4   volume  18734 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 878.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sp500_stooq = (pd.read_csv(data_path/'^spx_d.csv', index_col=0,\n",
    "                     parse_dates=True).loc['1950':'2024'].rename(columns=str.lower))\n",
    "print(sp500_stooq.info())\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sp500/stooq', sp500_stooq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a95143fb-5a79-4cae-be7e-7fcb9b929c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Security             GICS Sector               GICS Sub-Industry    Headquarters Location  Date added      CIK      Founded\n",
       "0    MMM           3M             Industrials        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04    66740         1902\n",
       "1    AOS  A. O. Smith             Industrials               Building Products     Milwaukee, Wisconsin  2017-07-26    91142         1916\n",
       "2    ABT       Abbott             Health Care           Health Care Equipment  North Chicago, Illinois  1957-03-04     1800         1888\n",
       "3   ABBV       AbbVie             Health Care                   Biotechnology  North Chicago, Illinois  2012-12-31  1551152  2013 (1888)\n",
       "4    ACN    Accenture  Information Technology  IT Consulting & Other Services          Dublin, Ireland  2011-07-06  1467373         1989"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "df = pd.read_html(url, header=0)[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314f8228-0d2f-450a-9fd0-f1694b1dfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['ticker', 'name', 'gics_sector', 'gics_sub_industry',\n",
    "              'location', 'first_added', 'cik', 'founded']\n",
    "df = df.set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "645a5f7a-2edb-4f17-afbb-51dac9fd7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 503 entries, MMM to ZTS\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   name               503 non-null    object\n",
      " 1   gics_sector        503 non-null    object\n",
      " 2   gics_sub_industry  503 non-null    object\n",
      " 3   location           503 non-null    object\n",
      " 4   first_added        503 non-null    object\n",
      " 5   cik                503 non-null    int64 \n",
      " 6   founded            503 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f66bb8-b521-4af7-b780-c6a99ffeeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sp500/stocks', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a5c4b4-3347-48d5-88e2-c986b6c5e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7176 entries, 0 to 7175\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ticker     7175 non-null   object\n",
      " 1   name       7176 non-null   object\n",
      " 2   lastsale   7176 non-null   object\n",
      " 3   netchange  7176 non-null   object\n",
      " 4   pctchange  7175 non-null   object\n",
      " 5   marketcap  6759 non-null   object\n",
      " 6   country    6878 non-null   object\n",
      " 7   ipoyear    4076 non-null   object\n",
      " 8   volume     7176 non-null   object\n",
      " 9   sector     6410 non-null   object\n",
      " 10  industry   6410 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 616.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path/'us_equities_meta_data.csv')\n",
    "df.columns = ['ticker', 'name', 'lastsale', 'netchange',\n",
    "              'pctchange', 'marketcap', 'country', 'ipoyear', 'volume', 'sector', 'industry']\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46a2632d-c2c4-49f9-8722-b385b0c6ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('us_equities/stocks', df.set_index('ticker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1b1b8c9-e227-4168-8c15-acbf3fda47f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n",
      "**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n",
      "**Please cite**:  \n",
      "\n",
      "The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n",
      "\n",
      "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n",
      "\n",
      "With some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n",
      "\n",
      "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "# ML dataset downloaded from openML.org\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89fc69a8-e434-4f4b-b69f-a9851a1d9898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "825bfc3e-95d0-4c89-91ed-110295a26dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = Path(data_path/'mnist')\n",
    "if not mnist_path.exists():\n",
    "    mnist_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fc8d467-774e-4732-901a-8e9342fe628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(mnist_path / 'data', mnist.data.astype(np.uint8))\n",
    "np.save(mnist_path / 'labels', mnist.target.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8e45e60-3e84-42d7-9912-0c32aca4493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = fetch_openml(name='Fashion-MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0a6b2f0-4150-46ca-bb71-027ec9b5e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Han Xiao, Kashif Rasul, Roland Vollgraf  \n",
      "**Source**: [Zalando Research](https://github.com/zalandoresearch/fashion-mnist)  \n",
      "**Please cite**: Han Xiao and Kashif Rasul and Roland Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, arXiv, cs.LG/1708.07747  \n",
      "\n",
      "Fashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. \n",
      "\n",
      "Raw data available at: https://github.com/zalandoresearch/fashion-mnist\n",
      "\n",
      "### Target classes\n",
      "Each training and test example is assigned to one of the following labels:\n",
      "Label  Description  \n",
      "0  T-shirt/top  \n",
      "1  Trouser  \n",
      "2  Pullover  \n",
      "3  Dress  \n",
      "4  Coat  \n",
      "5  Sandal  \n",
      "6  Shirt  \n",
      "7  Sneaker  \n",
      "8  Bag  \n",
      "9  Ankle boot\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print(fashion_mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df7c3636-d2bd-43d4-98ed-fc4362521522",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'T-shirt/top',\n",
    "              1: 'Trouser',\n",
    "              2: 'Pullover',\n",
    "              3: 'Dress',\n",
    "              4: 'Coat',\n",
    "              5: 'Sandal',\n",
    "              6: 'Shirt',\n",
    "              7: 'Sneaker',\n",
    "              8: 'Bag',\n",
    "              9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a84bacb-e025-4af9-8bbc-ffb41a074fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_path = Path(data_path/'fashion_mnist')\n",
    "if not fashion_path.exists():\n",
    "    fashion_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b098f305-3d1a-4954-bd1f-1818068032d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(label_dict).to_csv(fashion_path / 'label_dict.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc752eba-67d4-4c03-ba22-b956f49cbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(fashion_path / 'data', fashion_mnist.data.astype(np.uint8))\n",
    "np.save(fashion_path / 'labels', fashion_mnist.target.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca37ac-7a15-43c7-8bdf-cd153532321e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
